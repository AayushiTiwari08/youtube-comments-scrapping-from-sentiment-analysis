{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YtScrap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEWdsTW1PYaT"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# MINOR PROJECT \n",
        "\n",
        "                                                        \n",
        "\n",
        "#Sentiment Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6mEYadFQHqQ"
      },
      "source": [
        "#Importing the LIABRARIES which were needed to work on this project  \n",
        "\n",
        "Python library,visualisation library,NLTK library,regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luciqf-AHE0p",
        "outputId": "ef48c8d6-9ee0-4c6f-8e53-f204ed6f04d5"
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "#this NLTK package is used to perform cleaning of data\n",
        "#word_tonkenize is used to sperate  each words in the perform of list\n",
        "#sent_tonkenize is used to   seprate  each sentence in the sentnece  in the form of list\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation #punctutaion are . , all this\n",
        "from string import punctuation  # punctutaion are . , all this\n",
        "\n",
        "#stopwords package is used to remove the unneccessary words from the list \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#the wordcloud package in the python is used to generate the wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "import re\n",
        "import html\n",
        "\n",
        "#data visualisation library\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#this pandas library help to perform the dataframe\n",
        "import pandas as pd\n",
        "import itertools \n",
        "from collections import OrderedDict\n",
        "import collections\n",
        "\n",
        "#this perform the polarity of the sentiments\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline \n",
        "try:\n",
        "    nltk.download('stopwords')\n",
        "except:\n",
        "    pass\n"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDdk-BjwQk21"
      },
      "source": [
        "#Enter your APIs key "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oszi1nZXHzAg"
      },
      "source": [
        "api_key = 'your api key'\n",
        "\n"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ICc8Z7vQre3"
      },
      "source": [
        "#Making class object to work on youtube api\n",
        "#EXTRACING DATA FROM YOUTUBE COMMENTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx1ApcHZHbUl"
      },
      "source": [
        "youtube = build('youtube', 'v3',developerKey=api_key)\n",
        "video = youtube.commentThreads().list(part='snippet,replies',videoId='JKeRM918XDA').execute()\n",
        "comments = []\n",
        "while(video):\n",
        "    for item in range(len(video['items'])):\n",
        "        comments.append(video['items'][item]['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
        "    try:\n",
        "        if 'nextPageToken' in video and len(comments) <1001:\n",
        "            video = youtube.commentThreads().list(part = 'snippet',videoId = 'JKeRM918XDA' ,maxResults=50, pageToken=video['nextPageToken']).execute()\n",
        "        else:\n",
        "            break\n",
        "    except:\n",
        "        print(\"Not Much Comments\")\n",
        "\n",
        "data = ''.join(x for x in comments)\n"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdrsE8CDjeP_"
      },
      "source": [
        "data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgpJyNZekC9z"
      },
      "source": [
        "**Above result show the commnets extracted from the youtube video**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oClTBaWhQ6CU"
      },
      "source": [
        "#Cleaning data of youtube commnets \n",
        "#cleaning data with the help of package called REGULAR EXPRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGQD_6JPppp"
      },
      "source": [
        "all_comments_no_urls = [ \" \".join(re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"\", each_comment).split()) for each_comment in comments]\n",
        "\n",
        "all_comments_no_urls = [ \" \".join(re.sub('(@[A-Za-z0-9_]+)', \"\", each_comment).split()) for each_comment in comments]\n",
        "all_comments_no_urls = [ \" \".join(re.sub('<a.*?>|</a>', \"\", each_comment).split()) for each_comment in comments]\n",
        "all_comments_no_urls = [ \" \".join(re.sub('<br />','', each_comment).split()) for each_comment in comments]\n",
        "all_comments_no_urls = [ \" \".join(re.sub('</a>', '', each_comment).split()) for each_comment in comments]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# all_comments_no_urls = [ \" \".join(re.sub(re.sub('</a>', '', each_comment))).split() for each_comment in comments]\n",
        "\n",
        "\n"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf6KYg2HjiNX"
      },
      "source": [
        ""
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDK_W1NjRTnW"
      },
      "source": [
        "CONVERTING ALL COMMENTS TO THE LOWERCASE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39-y8X-oRVue"
      },
      "source": [
        "words_in_comments = [comm.lower().split() for comm in all_comments_no_urls]\n",
        "print(words_in_comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X_gemmtIZ-M"
      },
      "source": [
        "\n",
        "data = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', data)\n",
        "data = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", data)\n",
        "data = unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "data = re.sub('<a.*?>|</a> ', '', data)\n",
        "data = re.sub('<br />','',re.sub('</a>', '', data))\n"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMkhOqJ_WCTQ"
      },
      "source": [
        "#performing the part to get the wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kCrf352Itn3"
      },
      "source": [
        "\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\").generate(data)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQyArlPWWPwv"
      },
      "source": [
        "**The above image show the wordcloud of the commnets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLvN8ngpRxxH"
      },
      "source": [
        "#this section is counnting the words present in the previous list\n",
        "#words by words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yUVxjmkBbnC"
      },
      "source": [
        "\n",
        "# List of all words across commnet\n",
        "all_words_no_urls = list(itertools.chain(*words_in_comments))\n",
        "\n",
        "# Create counter\n",
        "counts_no_urls = collections.Counter(all_words_no_urls)\n",
        "\n",
        "counts_no_urls.most_common(12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IqS4lgbSMxb"
      },
      "source": [
        "#Counting comments in the form of dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3GUOUQBTeTa"
      },
      "source": [
        "clean_comments_no_urls = pd.DataFrame(counts_no_urls.most_common(15),\n",
        "                             columns=['words', 'count'])\n",
        "\n",
        "clean_comments_no_urls.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBcnpe0KSbvN"
      },
      "source": [
        "#Common words founnd in the data\n",
        "#Plotting the graph with the help of bar graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpIqjaqkT1U1"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Plot horizontal bar graph\n",
        "clean_comments_no_urls.sort_values(by='count').plot.barh(x='words',\n",
        "                      y='count',\n",
        "                      ax=ax,\n",
        "                      color=\"purple\")\n",
        "\n",
        "ax.set_title(\"Common Words Found in comment (Including All Words)\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ubEQG3pWpLS"
      },
      "source": [
        "**The above graph show the word chart including all words in the  comments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aD84MGJSsOV"
      },
      "source": [
        "#With the help of NLTK package we are now removing the stopwords from the commnets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiL2vBFmIh4o"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzHI1MY9-q9e"
      },
      "source": [
        "\n",
        "comments_with_no_sw= [[word for word in comment_words if not word in stop_words]\n",
        "              for comment_words in words_in_comments]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQqyk11KVYSZ"
      },
      "source": [
        "comments_with_no_sw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KExLo9m4TINY"
      },
      "source": [
        "#list of all words with no stopwords and with there count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWqBp1nOVtev"
      },
      "source": [
        "all_words_no_sw = list(itertools.chain(*comments_with_no_sw))\n",
        "\n",
        "counts_nsw = collections.Counter(all_words_no_sw)\n",
        "\n",
        "counts_nsw.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjp8t2jP_O4r"
      },
      "source": [
        "#Now performing the SENTIMENT PART\n",
        "**Sentiment like **\n",
        "\n",
        "**(NEGATIVE,POSTIVE , NETURAL)**\n",
        "\n",
        "\n",
        "#sentiment part with the help of package called TEXTBLOB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxw6FxgTCzNm"
      },
      "source": [
        "def get_commnet_sentiment(each_comm):\n",
        "        '''\n",
        "        Utility function to classify sentiment of passed tweet\n",
        "        using textblob's sentiment method\n",
        "        '''\n",
        "        # create TextBlob object of passed tweet text\n",
        "        analysis = TextBlob(each_comm)\n",
        "        # set sentiment\n",
        "        if analysis.sentiment.polarity > 0:\n",
        "            return 'positive'\n",
        "        elif analysis.sentiment.polarity == 0:\n",
        "            return 'neutral'\n",
        "        else:\n",
        "            return 'negative'"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEwUdMEAE9UV"
      },
      "source": [
        "TextBlob(\"The video is worst\").sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byHb4t_gRNgE"
      },
      "source": [
        "sentiment_score = []\n",
        "for i in range(len(comments)):\n",
        "  sentiment_score.append(get_commnet_sentiment(comments[i]))\n"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9hnelFAT6vb"
      },
      "source": [
        "#Output of sentiment of all commnets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SPzT8yEs0k1"
      },
      "source": [
        "sentiment_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCd7odRlUDYq"
      },
      "source": [
        "#Visualising the whole commnets data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_SrXXKRqfQr"
      },
      "source": [
        "scores = {'neutral':sentiment_score.count('neutral'), 'positive':sentiment_score.count('positive'), 'negative':sentiment_score.count('negative')}"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkphDIfSquPe"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFFOCRdeUXlX"
      },
      "source": [
        "The above output show the sentiment of the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkRn9HQJtOXv"
      },
      "source": [
        "scores.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOjuLddPtEdw"
      },
      "source": [
        "scores.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2yT-3WeokLF"
      },
      "source": [
        "plt.bar(scores.keys(), scores.values())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA1G-LfCUguE"
      },
      "source": [
        "The above garph show the sentiment of the video with the help of graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiIYe-NfUsZQ"
      },
      "source": [
        "**This overall project show the sentiment of any youtube video with the help of python and NLTK (natrual language processing) to get the REVIEWS of any youtube video(negative,positive,neutral)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcmHugMdqM_k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}